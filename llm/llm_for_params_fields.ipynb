{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2853998f-cad2-4696-ad06-e03941d7c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0098fb1c-8fbd-49a4-b5e8-02c052993387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9bcc62419042838fb4437d8d75707d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка модели LLM\n",
    "model_name = \"IlyaGusev/saiga_gemma2_9b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aabdc3c-383b-473a-891b-a5a3ee229e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подготовки промпта в формате Gemma-2\n",
    "def prepare_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"system\":\n",
    "            prompt += \"<start_of_turn>system\\n\" + content + \"<end_of_turn>\\n\"\n",
    "        elif role == \"user\":\n",
    "            prompt += \"<start_of_turn>user\\n\" + content + \"<end_of_turn>\\n\"\n",
    "        elif role == \"model\":\n",
    "            prompt += \"<start_of_turn>model\\n\" + content\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Функция для генерации ответа модели\n",
    "def generate_response(prompt, max_new_tokens=200):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    output = tokenizer.decode(output_ids[0][input_ids.shape[-1] :], skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634c3347-c4c2-4fb5-92bf-8b735cdf7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_parameters_prompt_data(\n",
    "    parameters_list: list[str],\n",
    "    group_name: str,\n",
    "    system_promt_group_parameters: str,\n",
    "    user_promt_group_parameters: str,\n",
    ") -> tuple[str]:\n",
    "    user_promt_group_parameters_formatted = user_promt_group_parameters.format(group_name, \"\\n\".join(parameters_list))\n",
    "\n",
    "    return system_promt_group_parameters, user_promt_group_parameters_formatted\n",
    "\n",
    "\n",
    "def process_message(text: str, system_prompt: str) -> str:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "    prompt = prepare_prompt(messages)\n",
    "    return generate_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f70e0ac-2db6-4204-85d7-848565c8063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtr = pl.read_parquet(\"../MTR.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a2bc656-7154-4917-ae98-fd4ac0764a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ПРОМТ ПОЛУЧЕНИЯ ПАРАМЕТРОВ\n",
    "system_promt_group_parameters = (\n",
    "    \"\"\"Ты — Сайга, русскоязычный ассистент. Ты помогаешь придумывать набор параметров для описания группы товаров\"\"\"\n",
    ")\n",
    "user_promt_group_parameters = \"\"\"Выдели из описаний набор параметров, которые позволят единым образом описать товары из группы с названием {}.\n",
    "\n",
    "ИНСТРУКЦИИ:\n",
    "1. Каждый параметр должен характеризоваться 1 словом.\n",
    "2. Набор параметров должен состоять не более чем из 10 параметров. \n",
    "3. Параметры должны основываться исключительно на информации из описаний.\n",
    "4. Если описания короткие и неинформативные, ты можешь вернуть менее, чем 10 параметров.\n",
    "5. Старайся понять, какие параметры отражают предоставленные описания товаров.\n",
    "6. Возвращай набор параметров как название каждого отдельного параметра с ; в качестве разделителя между ними\n",
    "7. Верни только набор параметров.\n",
    "\n",
    "ВХОДНЫЕ ДАННЫЕ (ОПИСАНИЯ ТОВАРОВ):\n",
    "Наименования и описания единиц товаров входящих в группу. Каждая пара будет начинаться с новой строчки и представлена в формате наименование товара: описание товара.\n",
    "{}\n",
    "\n",
    "ФОРМАТ ВЫВОДА:\n",
    "параметр 1; параметр 2; параметр 3; параметр n\n",
    "\n",
    "ПРИМЕР ВЫВОДА:\n",
    "длина; ширина; высота; цвет\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562708f2-5432-46d1-ac28-f4a3a9bee63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promt_group_parameters_formatted, user_promt_group_parameters_formatted = prepare_parameters_prompt_data(\n",
    "    mtr.head(4)[\"Параметры\"].to_numpy(), \"СОРОЧКА МУЖСКАЯ АО ФПК\", system_promt_group_parameters, user_promt_group_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c587faf5-5b18-42f6-a3ad-b9f73bf98643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model\\nтип; пол; категория; группа; должность; галуна; звезды'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_message(user_promt_group_parameters_formatted, system_promt_group_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
