{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1521918-325e-4492-af3e-2fbdf7aecd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Load the MTR dataset from a Parquet file\n",
    "mtr = pl.read_parquet(\"../MTR.parquet\")\n",
    "\n",
    "# Load the labels dataset from a Parquet file\n",
    "labels = pl.read_parquet(\"df_imputed_filled_labels.parquet\")\n",
    "\n",
    "# Join the MTR dataset with the labels dataset on the \"код СКМТР\" column\n",
    "data = mtr[[\"код СКМТР\", \"Наименование\"]].join(labels, on=\"код СКМТР\")\n",
    "\n",
    "# Prepare the texts for embedding by adding a prefix\n",
    "texts = [\"passage: \" + text for text in data[\"Наименование\"].to_numpy()]\n",
    "\n",
    "# Load the tokenizer and model from the pretrained multilingual-e5-small model\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
    "\n",
    "# Check if multiple GPUs are available and use them if so\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to('cuda')\n",
    "\n",
    "def get_embeddings(texts, batch_size=128):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given texts in batches.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of texts to generate embeddings for.\n",
    "        batch_size (int): Number of texts to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Concatenated embeddings for all texts.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "embeddings = get_embeddings(texts)\n",
    "\n",
    "# Create a DataFrame from the embeddings and save it to a Parquet file\n",
    "df = pl.DataFrame(embeddings.numpy())\n",
    "df.write_parquet(\"e5_small_emb.parquet\")\n",
    "\n",
    "# Add the \"ОКПД2\" and \"код СКМТР\" columns to the DataFrame for clustering\n",
    "df_for_clustering = df.with_columns(\n",
    "    mtr[\"ОКПД2\"].alias(\"ОКПД2\"),\n",
    "    mtr[\"код СКМТР\"].alias(\"код СКМТР\")\n",
    ")\n",
    "\n",
    "# Separate the DataFrame into rows with null and non-null \"ОКПД2\" values\n",
    "df_for_clustering_null = df_for_clustering.filter(\n",
    "    pl.col(\"ОКПД2\").is_null()\n",
    ")\n",
    "\n",
    "df_for_clustering_not_null = df_for_clustering.filter(\n",
    "    ~(pl.col(\"ОКПД2\").is_null())\n",
    ")\n",
    "\n",
    "# Load the label encoder and encode the \"ОКПД2\" column\n",
    "le = joblib.load('label_encoder_e5_clusters.pkl')\n",
    "df_encoded = df_for_clustering_not_null.with_columns(\n",
    "    pl.Series(le.transform(df_for_clustering_not_null[\"ОКПД2\"]), dtype=float).alias(\"ОКПД2\")\n",
    ")\n",
    "\n",
    "# Concatenate the encoded DataFrame with the null DataFrame\n",
    "all_data_encoded = pl.concat(\n",
    "    [\n",
    "        df_encoded, df_for_clustering_null.with_columns(\n",
    "            pl.Series(np.array([None]*len(df_for_clustering_null), dtype=np.float64)).alias(\"ОКПД2\")\n",
    "        )\n",
    "    ],\n",
    "    how=\"vertical\"\n",
    ")\n",
    "\n",
    "# Drop the \"код СКМТР\" column and convert the DataFrame to a NumPy array\n",
    "data = all_data_encoded.drop([\"код СКМТР\"]).to_numpy()\n",
    "\n",
    "# Impute missing values using KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "data_imputed = imputer.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame from the imputed data\n",
    "df_imputed = pl.DataFrame(data_imputed)\n",
    "df_imputed = pl.concat([df_imputed, all_data_encoded[[\"код СКМТР\"]]], how=\"horizontal\")\n",
    "\n",
    "# Get the unique \"ОКПД2\" values\n",
    "unique_okpd2 = df_imputed[\"column_384\"].unique().to_numpy()\n",
    "\n",
    "# Cluster the data for each unique \"ОКПД2\" value\n",
    "full_data = None\n",
    "for c, okpd2 in enumerate(tqdm(unique_okpd2)):\n",
    "    group_df = df_imputed.filter(pl.col(\"column_384\") == okpd2)\n",
    "    features = group_df.drop([\"column_384\", \"код СКМТР\"])\n",
    "\n",
    "    if len(features) < 2:\n",
    "        group_df = group_df[[\"column_384\", \"код СКМТР\"]].with_columns(\n",
    "            pl.Series([-1]*len(features)).alias(\"cluster\")\n",
    "        )\n",
    "    else:\n",
    "        if len(features) > 1000:\n",
    "            print(len(features))\n",
    "        hdbscan = HDBSCAN(min_cluster_size=2, n_jobs=-1, allow_single_cluster=True)\n",
    "        group_df = group_df[[\"column_384\", \"код СКМТР\"]].with_columns(\n",
    "            pl.Series(hdbscan.fit_predict(features)).alias(\"cluster\")\n",
    "        )\n",
    "\n",
    "    if c == 0:\n",
    "        full_data = group_df\n",
    "    else:\n",
    "        full_data = pl.concat([full_data, group_df], how=\"vertical\")\n",
    "\n",
    "# Save the clustered data to a Parquet file\n",
    "full_data.write_parquet(\"e5_small_clusters_v2.parquet\")\n",
    "\n",
    "# Load the label encoder and decode the \"ОКПД2\" column\n",
    "le = joblib.load('label_encoder_e5_clusters.pkl')\n",
    "full_data_clustered = full_data.with_columns(\n",
    "    pl.Series(\n",
    "        le.inverse_transform(\n",
    "            list(map(lambda x: int(x), full_data[\"column_384\"].to_list()))\n",
    "        )\n",
    "    ).alias(\"ОКПД2\")\n",
    ").drop([\"column_384\"])\n",
    "\n",
    "# Join the clustered data with the original MTR data and save it to a Parquet file\n",
    "full_data_clustered.join(mtr.drop([\"ОКПД2\"]), on=\"код СКМТР\").write_parquet(\"e5_small_clustered_data_v2.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
