{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df80e925-ef08-4124-a195-187c264094f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class ClusterPipe:\n",
    "    def __init__(self, mtr_path: str, labels_path: str, model_name: str = 'intfloat/multilingual-e5-small'):\n",
    "        \"\"\"\n",
    "        Initialize the Pipeline with paths to the MTR and labels datasets, and the model name.\n",
    "\n",
    "        Args:\n",
    "            mtr_path (str): Path to the MTR dataset.\n",
    "            labels_path (str): Path to the labels dataset.\n",
    "            model_name (str): Name of the pretrained model to use.\n",
    "        \"\"\"\n",
    "        self.mtr_path = mtr_path\n",
    "        self.labels_path = labels_path\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=[0, 1, 2, 3])\n",
    "        self.model.to('cuda')\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the MTR and labels datasets from the specified paths.\n",
    "        \"\"\"\n",
    "        self.mtr = pl.read_parquet(self.mtr_path)\n",
    "        self.labels = pl.read_parquet(self.labels_path)\n",
    "        self.data = self.mtr[[\"код СКМТР\", \"Наименование\"]].join(self.labels, on=\"код СКМТР\")\n",
    "\n",
    "    def prepare_texts(self) -> None:\n",
    "        \"\"\"\n",
    "        Prepare the texts for embedding by adding a prefix.\n",
    "        \"\"\"\n",
    "        self.texts = [\"passage: \" + text for text in self.data[\"Наименование\"].to_numpy()]\n",
    "\n",
    "    def get_embeddings(self, batch_size: int = 128) -> None:\n",
    "        \"\"\"\n",
    "        Generate embeddings for the given texts in batches.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of texts to process in each batch.\n",
    "        \"\"\"\n",
    "        all_embeddings: list[Tensor] = []\n",
    "        for i in tqdm(range(0, len(self.texts), batch_size)):\n",
    "            batch_texts = self.texts[i:i + batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "        self.embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    def save_embeddings(self, output_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the embeddings to a Parquet file.\n",
    "\n",
    "        Args:\n",
    "            output_path (str): Path to save the embeddings.\n",
    "        \"\"\"\n",
    "        df = pl.DataFrame(self.embeddings.numpy())\n",
    "        df.write_parquet(output_path)\n",
    "\n",
    "    def load_embeddings(self, input_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load the embeddings from a Parquet file.\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Path to load the embeddings from.\n",
    "        \"\"\"\n",
    "        self.embeddings = pl.read_parquet(input_path).to_numpy()\n",
    "\n",
    "    def prepare_for_clustering(self) -> None:\n",
    "        \"\"\"\n",
    "        Prepare the data for clustering by adding the \"ОКПД2\" and \"код СКМТР\" columns.\n",
    "        \"\"\"\n",
    "        self.df_for_clustering = pl.DataFrame(self.embeddings).with_columns(\n",
    "            self.mtr[\"ОКПД2\"].alias(\"ОКПД2\"),\n",
    "            self.mtr[\"код СКМТР\"].alias(\"код СКМТР\")\n",
    "        )\n",
    "        self.df_for_clustering_null = self.df_for_clustering.filter(pl.col(\"ОКПД2\").is_null())\n",
    "        self.df_for_clustering_not_null = self.df_for_clustering.filter(~(pl.col(\"ОКПД2\").is_null()))\n",
    "\n",
    "    def encode_labels(self, encoder_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Encode the \"ОКПД2\" column using a label encoder.\n",
    "\n",
    "        Args:\n",
    "            encoder_path (str): Path to the label encoder.\n",
    "        \"\"\"\n",
    "        le = joblib.load(encoder_path)\n",
    "        self.df_encoded = self.df_for_clustering_not_null.with_columns(\n",
    "            pl.Series(le.transform(self.df_for_clustering_not_null[\"ОКПД2\"]), dtype=float).alias(\"ОКПД2\")\n",
    "        )\n",
    "        self.all_data_encoded = pl.concat(\n",
    "            [\n",
    "                self.df_encoded, self.df_for_clustering_null.with_columns(\n",
    "                    pl.Series(np.array([None]*len(self.df_for_clustering_null), dtype=np.float64)).alias(\"ОКПД2\")\n",
    "                )\n",
    "            ],\n",
    "            how=\"vertical\"\n",
    "        )\n",
    "\n",
    "    def impute_missing_values(self) -> None:\n",
    "        \"\"\"\n",
    "        Impute missing values using KNNImputer.\n",
    "        \"\"\"\n",
    "        data = self.all_data_encoded.drop([\"код СКМТР\"]).to_numpy()\n",
    "        imputer = KNNImputer(n_neighbors=1)\n",
    "        self.data_imputed = imputer.fit_transform(data)\n",
    "        self.df_imputed = pl.DataFrame(self.data_imputed)\n",
    "        self.df_imputed = pl.concat([self.df_imputed, self.all_data_encoded[[\"код СКМТР\"]]], how=\"horizontal\")\n",
    "\n",
    "    def cluster_data(self, output_path: str, encoder_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Cluster the data for each unique \"ОКПД2\" value and save the clustered data to a Parquet file.\n",
    "\n",
    "        Args:\n",
    "            output_path (str): Path to save the clustered data.\n",
    "            encoder_path (str): Path to the label encoder.\n",
    "        \"\"\"\n",
    "        unique_okpd2 = self.df_imputed[\"column_384\"].unique().to_numpy()\n",
    "        full_data = None\n",
    "        for c, okpd2 in enumerate(tqdm(unique_okpd2)):\n",
    "            group_df = self.df_imputed.filter(pl.col(\"column_384\") == okpd2)\n",
    "            features = group_df.drop([\"column_384\", \"код СКМТР\"])\n",
    "\n",
    "            if len(features) < 2:\n",
    "                group_df = group_df[[\"column_384\", \"код СКМТР\"]].with_columns(\n",
    "                    pl.Series([-1]*len(features)).alias(\"cluster\")\n",
    "                )\n",
    "            else:\n",
    "                if len(features) > 1000:\n",
    "                    print(len(features))\n",
    "                hdbscan = HDBSCAN(min_cluster_size=2, n_jobs=-1, allow_single_cluster=True)\n",
    "                group_df = group_df[[\"column_384\", \"код СКМТР\"]].with_columns(\n",
    "                    pl.Series(hdbscan.fit_predict(features)).alias(\"cluster\")\n",
    "                )\n",
    "\n",
    "            if c == 0:\n",
    "                full_data = group_df\n",
    "            else:\n",
    "                full_data = pl.concat([full_data, group_df], how=\"vertical\")\n",
    "\n",
    "        full_data.write_parquet(output_path)\n",
    "\n",
    "        le = joblib.load(encoder_path)\n",
    "        self.full_data_clustered = full_data.with_columns(\n",
    "            pl.Series(\n",
    "                le.inverse_transform(\n",
    "                    list(map(lambda x: int(x), full_data[\"column_384\"].to_list()))\n",
    "                )\n",
    "            ).alias(\"ОКПД2\")\n",
    "        ).drop([\"column_384\"])\n",
    "\n",
    "    def save_clustered_data(self, output_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the clustered data to a Parquet file.\n",
    "\n",
    "        Args:\n",
    "            output_path (str): Path to save the clustered data.\n",
    "        \"\"\"\n",
    "        self.full_data_clustered.join(self.mtr.drop([\"ОКПД2\"]), on=\"код СКМТР\").write_parquet(output_path)\n",
    "\n",
    "    def load_clustered_data(self, input_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load the clustered data from a Parquet file.\n",
    "\n",
    "        Args:\n",
    "            input_path (str): Path to load the clustered data from.\n",
    "        \"\"\"\n",
    "        self.full_data_clustered = pl.read_parquet(input_path).fill_null(\"\")\n",
    "\n",
    "    def sample_cluster(self, cluster_num: int, okpd_identifier: str) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter the clustered data to return a DataFrame containing only the rows that belong to a specific cluster and OKPD2 identifier.\n",
    "    \n",
    "        Args:\n",
    "            cluster_num (int): The cluster number to filter by.\n",
    "            okpd_identifier (str): The OKPD2 identifier to filter by.\n",
    "    \n",
    "        Returns:\n",
    "            pl.DataFrame: A DataFrame containing the rows that match the specified cluster number and OKPD2 identifier.\n",
    "        \"\"\"\n",
    "        return self.full_data_clustered.filter(\n",
    "            (pl.col(\"cluster\") == cluster_num) & (pl.col(\"ОКПД2\") == okpd_identifier)\n",
    "        )\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initialize the LLMParamsFields class with the specified model name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the LLM model to load.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "    def prepare_prompt(self, messages: list[dict]) -> str:\n",
    "        \"\"\"\n",
    "        Prepare the prompt in the format required by the Gemma-2 model.\n",
    "\n",
    "        Args:\n",
    "            messages (List[dict]): A list of messages with roles and content.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted prompt.\n",
    "        \"\"\"\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            role = message['role']\n",
    "            content = message['content']\n",
    "            if role == 'system':\n",
    "                prompt += \"<start_of_turn>system\\n\" + content + \"<end_of_turn>\\n\"\n",
    "            elif role == 'user':\n",
    "                prompt += \"<start_of_turn>user\\n\" + content + \"<end_of_turn>\\n\"\n",
    "            elif role == 'model':\n",
    "                prompt += \"<start_of_turn>model\\n\" + content\n",
    "        return prompt\n",
    "\n",
    "    def generate_response(self, prompt: str, max_new_tokens: int = 200) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the LLM model based on the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt.\n",
    "            max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids.to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        output = self.tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        return output.strip()\n",
    "\n",
    "\n",
    "    def process_message(self, text: str, system_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Process a message to generate a response from the LLM model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The user input text.\n",
    "            system_prompt (str): The system prompt.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "\n",
    "        prompt = self.prepare_prompt(messages)\n",
    "        return self.generate_response(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4b6fb7-d2ac-4cf7-b667-c5976fa64ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineManager:\n",
    "    def __init__(self, cluster_pipe: ClusterPipe, llm: LLM):\n",
    "        \"\"\"\n",
    "        Initialize the PipelineManager with paths to the MTR and labels datasets, and the model names.\n",
    "\n",
    "        Args:\n",
    "            llm (LLM)\n",
    "            cluster_pipe (ClusterPipe)\n",
    "        \"\"\"\n",
    "        self.cluster_pipe = cluster_pipe\n",
    "        self.llm = llm\n",
    "\n",
    "    def get_cluster(self, cluster_num: int, okpd_identifier: str) -> pl.DataFrame:\n",
    "        return cluster_pipe.sample_cluster(cluster_num, okpd_identifier)\n",
    "\n",
    "    def get_cluster_name(self, cluster: pl.DataFrame) -> str:\n",
    "        system_promt_cluster_name = \"\"\"Ты русскоязычный ассистент. Ты помогаешь придумывать название для группы товаров\"\"\"\n",
    "        user_promt_cluster_name = \"\"\"Придуймай обобщающее название для группы товаров.\n",
    "        ИНСТРУКЦИИ:\n",
    "        1. Название должно быть четким и напрямую связанным с группой товаров.\n",
    "        2. Формат названия: существительное (существительное/прилагательное).\n",
    "        3. Верни только название в описанном формате.\n",
    "        ВХОДНЫЕ ДАННЫЕ:\n",
    "        Наименования единиц товаров входящих в группу. Каждое наименование написано с новой строчки.\n",
    "        Наименования товаров:\n",
    "        {}.\n",
    "        ФОРМАТЫ ВЫВОДА:\n",
    "        существительное\n",
    "        существительное прилагательное\n",
    "        существительное (прилагательные)\n",
    "        ПРИМЕРЫ ВЫВОДА:\n",
    "        шланги (садовые)\n",
    "        камеры ночные\n",
    "        \"\"\"\n",
    "        \n",
    "        products = cluster[\"Наименование\"].sample(fraction=1).to_list()[:16]\n",
    "        cluster_name = self.llm.process_message(system_promt_cluster_name, user_promt_cluster_name.format(products)).split(\"\\n\")[1]\n",
    "        return cluster_name\n",
    "        \n",
    "    def gen_json_str_from_params_list(self, params_list: list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generates a JSON string from a list of parameter names.\n",
    "    \n",
    "        Args:\n",
    "        params_list (list of str): List of parameter names.\n",
    "    \n",
    "        Returns:\n",
    "        str: JSON string with parameters and their corresponding values.\n",
    "        \"\"\"\n",
    "        params_dict = {param: f\"value{i+1}\" for i, param in enumerate(params_list)}\n",
    "        json_str = json.dumps(params_dict, ensure_ascii=False, indent=4)\n",
    "    \n",
    "        return json_str\n",
    "\n",
    "\n",
    "    def parse_product_properties(self, row: dict, cluster_properties: str, cluster_name: str) -> str:\n",
    "        system_promt_extract_params_from = \"\"\"Ты — Сайга, русскоязычный ассистент. Ты извлекаешь свойства товаров из описания параметров.\"\"\"\n",
    "        user_promt_struct_params = \"\"\"Извлеки свойства товара из описания параметров товара следующей группы {}.\n",
    "    \n",
    "        ИНСТРУКЦИИ:\n",
    "        1. Извлекай параметры из описания в соответствии с определенной структурой. Данная структура представляет собой набор параметров в формате json.\n",
    "        2. Для каждого параметра из структуры найди соответсвующее ему значение в описании.\n",
    "        3. Если ты не можешь найти значение параметра в описании, то заполняй этот параметр значением NODATA, но только при действительном отсутствии параметра.\n",
    "        4. Выводи только json файл с точно такой же структурой и заполненными значениями параметров.\n",
    "    \n",
    "        СТРУКТУРА ПАРАМЕТРОВ И СТРУКТУРА ОТВЕТА:\n",
    "        json\n",
    "        {}\n",
    "    \n",
    "        Заполни ее значениями параметров из следующего текста.\n",
    "    \n",
    "        ОПИСАНИЕ ПАРАМЕТРОВ ТОВАРА:\n",
    "        {}\n",
    "        \"\"\"\n",
    "        \n",
    "        json_str_from_params_list = self.gen_json_str_from_params_list(\n",
    "        cluster_properties.split(\";\")\n",
    "        )\n",
    "        \n",
    "        params_raw = f\"{row['Наименование']} {row['Маркировка']}: {row['Параметры']}\"\n",
    "        \n",
    "        params_parsed = self.llm.process_message(\n",
    "        user_promt_struct_params.format(cluster_name, json_str_from_params_list, params_raw),\n",
    "            system_promt_extract_params_from\n",
    "        )\n",
    "        start_index = params_parsed.find('{')\n",
    "        end_index = params_parsed.rfind('}') + 1\n",
    "        cleaned_json_string = params_parsed[start_index:end_index]\n",
    "        return cleaned_json_string\n",
    "\n",
    "    \n",
    "    def get_cluster_properties(self,\n",
    "                               cluster: pl.DataFrame,\n",
    "                               cluster_name: str) -> str:\n",
    "        system_promt_group_parameters = \"\"\"Ты — Сайга, русскоязычный ассистент. Ты помогаешь придумывать набор параметров для описания группы товаров\"\"\"\n",
    "        user_promt_group_parameters = \"\"\"Выдели из описаний набор параметров, которые позволят единым образом описать товары из группы с названием \"{}\".\n",
    "    \n",
    "        ИНСТРУКЦИИ:\n",
    "        1. Каждый параметр должен характеризоваться 1 словом.\n",
    "        2. Набор параметров должен состоять не более чем из 10 параметров. \n",
    "        3. Параметры должны основываться исключительно на информации из описаний.\n",
    "        4. Если описания короткие и неинформативные, ты можешь вернуть менее, чем 10 параметров.\n",
    "        5. Старайся понять, какие параметры отражают предоставленные описания товаров.\n",
    "        6. Обращай внимание на цифры и предполагай, какой параметр они могут означать в данном контексте.\n",
    "        7. Делай параметры разнообразными и не дублирующими друг друга по смыслу.\n",
    "        8. Возвращай набор параметров как название каждого отдельного параметра с ; в качестве разделителя между ними.\n",
    "        9. Верни только набор параметров.\n",
    "    \n",
    "        ВХОДНЫЕ ДАННЫЕ (ОПИСАНИЯ ТОВАРОВ):\n",
    "        Наименования и описания единиц товаров входящих в группу. Каждая пара будет начинаться с новой строчки и представлена в формате наименование товара: описание товара.\n",
    "        {}\n",
    "    \n",
    "        ФОРМАТ ВЫВОДА:\n",
    "        параметр 1; параметр 2; параметр 3; параметр n\n",
    "    \n",
    "        ПРИМЕР ВЫВОДА:\n",
    "        длина; ширина; высота; цвет\n",
    "        \"\"\"\n",
    "        if len(cluster) > 2:\n",
    "            products = cluster.sample(fraction=1).sample(len(cluster)//2)\n",
    "        else: \n",
    "            products = cluster.head(1)\n",
    "            \n",
    "        products = \"\\n\".join([\n",
    "                name + \" \" + params + \": \" + mark for name, params, mark in zip(\n",
    "                    products[\"Наименование\"].to_numpy(),\n",
    "                    products[\"Маркировка\"].to_numpy(),\n",
    "                    products[\"Параметры\"].to_numpy(),\n",
    "                )\n",
    "            ])\n",
    "        \n",
    "        properties = self.llm.process_message(\n",
    "            user_promt_group_parameters.format(cluster_name, products), system_promt_group_parameters\n",
    "        ).split(\"\\n\")[1]\n",
    "        \n",
    "        return properties\n",
    "\n",
    "    def cluster_process(self, cluster_sample: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Process a cluster sample to generate cluster name, properties, and parse product properties.\n",
    "\n",
    "        Args:\n",
    "            cluster_sample (pl.DataFrame): DataFrame containing the cluster data.\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame: DataFrame with added cluster name, properties, and parsed item properties.\n",
    "        \"\"\"\n",
    "        cluster_name = self.get_cluster_name(cluster_sample)\n",
    "        cluster_prop = self.get_cluster_properties(cluster_sample, cluster_name)\n",
    "\n",
    "        answer = cluster_sample.with_columns(\n",
    "            pl.lit(cluster_name).alias(\"cluster_name\"),\n",
    "            pl.lit(cluster_prop).alias(\"cluster_properties\")\n",
    "        )\n",
    "\n",
    "        items_jsons = []\n",
    "        for item in answer.iter_rows(named=True):\n",
    "            item_prop = self.parse_product_properties(item, cluster_prop, cluster_name)\n",
    "            items_jsons.append(item_prop)\n",
    "\n",
    "        group = answer.with_columns(\n",
    "            pl.Series(items_jsons).alias(\"parsed_item_properties\")\n",
    "        )\n",
    "\n",
    "        products = []\n",
    "        for row in group.iter_rows(named=True):\n",
    "            product = {}\n",
    "            product[\"код СКМТР\"] = row[\"код СКМТР\"]\n",
    "            product[\"Наименование\"] = row[\"Наименование\"]\n",
    "            product[\"Маркировка\"] = row[\"Маркировка\"]\n",
    "            product[\"Группа\"] = row[\"cluster_name\"]\n",
    "            product[\"ОКПД2\"] = row[\"ОКПД2\"]\n",
    "            product.update(json.loads( row[\"parsed_item_properties\"]))\n",
    "            products.append(product)\n",
    "\n",
    "        return pl.DataFrame(products).fill_null(\"NODATA\")\n",
    "\n",
    "    def process_okpd(self, okpd_ident: str) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Process all clusters for a given OKPD2 identifier.\n",
    "\n",
    "        Args:\n",
    "            okpd_ident (str): The OKPD2 identifier to process.\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame: DataFrame containing the processed clusters for the given OKPD2 identifier.\n",
    "        \"\"\"\n",
    "        okpd_df = self.cluster_pipe.full_data_clustered.filter(\n",
    "            pl.col(\"ОКПД2\") == okpd_ident\n",
    "        )\n",
    "\n",
    "        unique_clusters = okpd_df[\"cluster\"].unique()\n",
    "\n",
    "        for c, cluster_id in enumerate(unique_clusters):\n",
    "            cluster_sample = self.cluster_pipe.sample_cluster(cluster_id, okpd_ident)\n",
    "            processed_cluster = self.cluster_process(cluster_sample)\n",
    "\n",
    "            if c == 0:\n",
    "                full_data = processed_cluster\n",
    "            else:\n",
    "                full_data = pl.concat([full_data, processed_cluster], how=\"vertical\")\n",
    "\n",
    "        return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6545b88c-aa9b-402b-8bf9-403d2f0fd927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c7026f9c5f4c969e08ddd6ba59ee11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_pipe = ClusterPipe(mtr_path=\"../MTR.parquet\", labels_path=\"df_imputed_filled_labels.parquet\")\n",
    "# pipeline.load_data()\n",
    "# pipeline.prepare_texts()\n",
    "# pipeline.get_embeddings()\n",
    "# pipeline.save_embeddings(\"e5_small_emb.parquet\")\n",
    "# pipeline.load_embeddings(\"e5_small_emb.parquet\")\n",
    "# pipeline.prepare_for_clustering()\n",
    "# pipeline.encode_labels('label_encoder_e5_clusters.pkl')\n",
    "# pipeline.impute_missing_values()\n",
    "# pipeline.cluster_data(\"e5_small_clusters_v2.parquet\", 'label_encoder_e5_clusters.pkl')\n",
    "# pipeline.save_clustered_data(\"e5_small_clustered_data_v2.parquet\")\n",
    "cluster_pipe.load_clustered_data(\"e5_small_clustered_data_v2.parquet\")\n",
    "\n",
    "model_name = 'IlyaGusev/saiga_gemma2_9b'\n",
    "llm = LLM(model_name)\n",
    "\n",
    "pipe = PipelineManager(cluster_pipe, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00168fbb-1f3f-4dba-bb16-6af90933edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сырники (со сметанным, со сгущенным молоком, из творога со сметанным)\n",
      "наименование; вес; состав; тип_начинки; размер; количество_порций\n",
      "{\n",
      "    \"наименование\": \"Сырники со сметанным\",\n",
      "    \"вес\": \"150/40\",\n",
      "    \"состав\": \"из творога со сметанным\",\n",
      "    \"тип_начинки\": \"со сметанным\",\n",
      "    \"размер\": \"NODATA\",\n",
      "    \"количество_порций\": \"NODATA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cluster_sample = pipe.get_cluster(9, \"10.20.25.190\")\n",
    "cluster_name = pipe.get_cluster_name(cluster_sample)\n",
    "cluster_prop = pipe.get_cluster_properties(cluster_sample, cluster_name)\n",
    "item_prop = pipe.parse_product_properties(\n",
    "    cluster_sample.head(1).to_dicts()[0], cluster_prop, cluster_name\n",
    ")\n",
    "\n",
    "print(cluster_name)\n",
    "print(cluster_prop)\n",
    "print(item_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a53a80-6d4f-44bd-b075-1e76f156c1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>код СКМТР</th><th>Наименование</th><th>Маркировка</th><th>Группа</th><th>ОКПД2</th><th>наименование</th><th>состав</th><th>вес</th><th>размер</th><th>тип</th><th>вкус</th><th>форма</th><th>упаковка</th><th>срок годности</th><th>цена</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;9266220119&quot;</td><td>&quot;СЫРНИКИ СО СМЕТАНОЙ&quot;</td><td>&quot;&quot;</td><td>&quot;сырники (со сметанным, со сгущ…</td><td>&quot;10.20.25.190&quot;</td><td>&quot;Сырники со сметанным&quot;</td><td>&quot;Из творога со сметанным&quot;</td><td>&quot;150/40&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td></tr><tr><td>&quot;9266220121&quot;</td><td>&quot;СЫРНИКИ СО СГУЩЕНЫМ МОЛОКОМ&quot;</td><td>&quot;&quot;</td><td>&quot;сырники (со сметанным, со сгущ…</td><td>&quot;10.20.25.190&quot;</td><td>&quot;Сырники со сгущенным молоком&quot;</td><td>&quot;Из творога со сгущенным молоко…</td><td>&quot;75/20Г&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td></tr><tr><td>&quot;9266220128&quot;</td><td>&quot;СЫРНИКИ ИЗ ТВОРОГА СО СМЕТАНОЙ&quot;</td><td>&quot;&quot;</td><td>&quot;сырники (со сметанным, со сгущ…</td><td>&quot;10.20.25.190&quot;</td><td>&quot;Сырники из творога со сметаной&quot;</td><td>&quot;Из творога со сметаной&quot;</td><td>&quot;150/20г&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td><td>&quot;NODATA&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 15)\n",
       "┌────────────┬─────────────┬────────────┬────────────┬───┬────────┬──────────┬────────────┬────────┐\n",
       "│ код СКМТР  ┆ Наименовани ┆ Маркировка ┆ Группа     ┆ … ┆ форма  ┆ упаковка ┆ срок       ┆ цена   │\n",
       "│ ---        ┆ е           ┆ ---        ┆ ---        ┆   ┆ ---    ┆ ---      ┆ годности   ┆ ---    │\n",
       "│ str        ┆ ---         ┆ str        ┆ str        ┆   ┆ str    ┆ str      ┆ ---        ┆ str    │\n",
       "│            ┆ str         ┆            ┆            ┆   ┆        ┆          ┆ str        ┆        │\n",
       "╞════════════╪═════════════╪════════════╪════════════╪═══╪════════╪══════════╪════════════╪════════╡\n",
       "│ 9266220119 ┆ СЫРНИКИ СО  ┆            ┆ сырники    ┆ … ┆ NODATA ┆ NODATA   ┆ NODATA     ┆ NODATA │\n",
       "│            ┆ СМЕТАНОЙ    ┆            ┆ (со        ┆   ┆        ┆          ┆            ┆        │\n",
       "│            ┆             ┆            ┆ сметанным, ┆   ┆        ┆          ┆            ┆        │\n",
       "│            ┆             ┆            ┆ со сгущ…   ┆   ┆        ┆          ┆            ┆        │\n",
       "│ 9266220121 ┆ СЫРНИКИ СО  ┆            ┆ сырники    ┆ … ┆ NODATA ┆ NODATA   ┆ NODATA     ┆ NODATA │\n",
       "│            ┆ СГУЩЕНЫМ    ┆            ┆ (со        ┆   ┆        ┆          ┆            ┆        │\n",
       "│            ┆ МОЛОКОМ     ┆            ┆ сметанным, ┆   ┆        ┆          ┆            ┆        │\n",
       "│            ┆             ┆            ┆ со сгущ…   ┆   ┆        ┆          ┆            ┆        │\n",
       "│ 9266220128 ┆ СЫРНИКИ ИЗ  ┆            ┆ сырники    ┆ … ┆ NODATA ┆ NODATA   ┆ NODATA     ┆ NODATA │\n",
       "│            ┆ ТВОРОГА СО  ┆            ┆ (со        ┆   ┆        ┆          ┆            ┆        │\n",
       "│            ┆ СМЕТАНОЙ    ┆            ┆ сметанным, ┆   ┆        ┆          ┆            ┆        │\n",
       "│            ┆             ┆            ┆ со сгущ…   ┆   ┆        ┆          ┆            ┆        │\n",
       "└────────────┴─────────────┴────────────┴────────────┴───┴────────┴──────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_sample = pipe.get_cluster(9, \"10.20.25.190\")\n",
    "pipe.cluster_process(cluster_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd527636-eed7-4152-a710-87ef6c2b9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.cluster_process(cluster_sample).write_excel(\"answer_dump.xsls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
